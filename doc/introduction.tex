\section{introduction}
\subsection{What kind of document is this?}
This document was written mainly before we started building Arakoon. 
It served the purpose as a basis of discussion, 
trying to get consensus on what we were going to build, before we build it. 
We regularly come back to this document to record decisions, and to keep it anchored to reality. Currently, it's not very well structured, the focus being on \emph{getting the information in there first}. 
Hopefully there will eventually be time to clean it up.
\subsection{why Arakoon?}
We have been using several distributed non relational data stores for a long time now, and it has not been a satisfying experience.
\subsection{what we aim for}
We want a simple distributed key/value store that is easy to understand and use.
We don't need infinite scalability (and in fact we have several limitations), but we do have some requests regarding
\paragraph{consistency}
The system as a whole needs to provide a consistent view on the distributed state.
This stems from the experience that eventual consistency is too heavy a burden for a user application to manage. 
A simple example is a retrieval of the value for a key where you might receive none, one or multiple values depending on the weather conditions. The next question is always: Why don't a get a result? Is it because there is no value, or merely because I currently cannot retreive it?

\paragraph{conditional atomic updates}
We don't need full blown transactions (would be nice to have though), 
but we do need updates that abort if the state is not what we expect it to be.
So at least an atomic conditional update and an atomic multi-update are needed.
\paragraph{robustness}
The system must be able to cope with failure of individual components, without concessions to consistency.
However, whenever consistency can no longer be guaranteed, updates must simply fail.
\paragraph{locality control}
When we deploy a system over 2 datacenters, we want guarantees that the entire state is indeed present in both datacenters. (This is something we could not get from distributed hash tables using consistent hashing)

\paragraph{healing/recovery}
Whenever a component dies and is subsequently revived, or replaced the system must be able to guide that component towards a situation where that node again fully participates. 
If this cannot be done fully automatically, then human intervention should be trivial.
\paragraph{explicit failure}
Whenever there is something wrong, failure should propagate quite quickly.
This in contrast to systems that keep on trying to remedy the situation themselves al the time.
\subsection{Isn't this what Keyspace does?}
Almost. We used keyspace for a while but were struggling with some issues.

\paragraph{atomic multi updates}
With Keyspace, you can do multiple updates in one request, 
but it's nowhere near atomic, so when one of them fails, you're in limbo. Arakoon supports a sequence update which is an all or nothing thing.
\paragraph{test\_and\_set}
This is a conditional update, that only changes a value for a key when the store has the expected state.
Keyspace supports this, but not when there is no expected value. 
This makes it impossible to atomically set a value only if it was not present.
An Arakoon \emph{test\_and\_set} can be used to set a new value, 
update an existing value, or remove an existing value.
It's also important to notice Arakoon returns the \emph{old} value after a
\emph{test\_and\_set}, allowing one to determine whether an update took place
\paragraph{Keyspace defects}
\begin{itemize}

\item{} Segmentation violations under heavy load.
\item{} Accidental semantic changes of the interfaces
\item{} Keyspace nodes happily accept data even when the disk is full.
\item{} Several cluster in limbo (no updates possible) scenarios.
\item{} abandoned python client. There used to be a pure python client, 
but now it's abandoned in favour of a \emph{SWIGed around C} library, 
which enforces it's own reconnection strategy.
\item{} Berkeley DB.\footnote{May, 2011, Scalien dropped Keyspace due to \emph{'BerkeleyDB issues'}} 

\end{itemize}
It's not that these issues are impossible to address. It's just that when you're fighting these, you don't want to be owner of the problem, not a spectator.
\subsection{high level overview}
Arakoon deployments consist of a small collection of \emph{nodes} (typically 1,2,3 or 5 nodes) that keep replicas of key/values, and \emph{clients} that manipulate the key/value space.
In principle, all nodes have the entire key/value space.
There is one distinguished node called the master with which all clients communicate to perform updates.
A client contacts any node to find out the master, and then just conversates with the master.
If a master dies, a new one is elected automatically, and clients fail over to that master.
A slave node is a node that is not master.
A node that is not up-to-date cannot become master.

\subsection{limitations}
\paragraph{capacity}
Since all nodes store the entire space, the capacity of the smallest node limits the system.
\paragraph{number of clients}
Since all updates go through the master, the system is not suited for large amounts of concurrent clients.
\paragraph{opaque values}
The system does not really understand the values, and hence cannot do validation, or transformations \ldots
\paragraph{hidden master failure}
If the key value store on the master silently corrupts, gets will be affected.

